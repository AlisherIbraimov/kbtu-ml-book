{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voting, stacking and blending\n",
    "\n",
    "Ensembling techniques like boosting, stacking, and blending are powerful strategies for combining the predictions of multiple base models to improve overall model performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{image} https://miro.medium.com/v2/resize:fit:1200/1*tWdplltkOLK8gY46F-1GiQ.jpeg\n",
    ":alt: stacking\n",
    ":class: bg-primary mb-1\n",
    ":width: 500px\n",
    ":align: center\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. <h4>Voting:</h4>\n",
    "\n",
    "    A voting ensemble method is a way of making predictions in machine learning by combining the results from multiple different models. Imagine you have a group of friends and you want to decide on a movie to watch. Each friend has their own preferences and suggestions. In a voting ensemble, each \"friend\" (or model) gets a vote on what movie to watch, and the movie with the most votes is chosen.\n",
    "    \n",
    "* Strengths: Voting method is easy to implement. They're more robust because they consider multiple models, reducing the impact of mistakes made by single models, it results as better accuracy. \n",
    "    \n",
    "* Weaknesses: It may not capture complex interactions between base models as effectively as stacking. The choice of the  combining rule is critical.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The output of regression tasks is the average of the models’ predictions. Instead, there are two methods for predicting the outcome of classification tasks: **Hard voting** and **Soft voting**.<br>\n",
    "     * **Hard Voting**: Each model in the ensemble gives a prediction, and the final prediction is determined by a majority vote.\n",
    "     * **Soft Voting**: The final prediction is based on the average probability of outcomes from different models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{image} https://www.mdpi.com/applsci/applsci-12-07554/article_deploy/html/images/applsci-12-07554-g003-550.jpg\n",
    ":alt: voting_types\n",
    ":class: bg-primary mb-1\n",
    ":width: 500px\n",
    ":align: center\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. <h4>Stacking (Stacked Generalization):</h4>\n",
    "\n",
    "* Basic Idea: Stacking is an ensemble method that combines the predictions of multiple base models by training a meta-model (also called a meta-learner) on top of them. The meta-model learns to weigh the predictions of the base models. Here's how it works:\n",
    "\n",
    "    * First, you need to train your base models on the original data. These models are considered weak learners as they don't necessarily have to provide the most accurate predictions.\n",
    "    * Next, you need to prepare a new dataset where each row is a base model's prediction.\n",
    "\n",
    "    * Finally, you train your meta-model on this new dataset. The meta-model learns to weigh the predictions of the base models to make better predictions on unseen data\n",
    "\n",
    "Here's image that illustrates it:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{image} https://cdn.analyticsvidhya.com/wp-content/uploads/2021/03/Screenshot-from-2021-03-30-15-07-08.png\n",
    ":alt: stacking\n",
    ":class: bg-primary mb-1\n",
    ":width: 500px\n",
    ":align: center\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking offers several advantages over traditional ensemble methods:\n",
    "\n",
    "  * It can capture complex relationships between base models and often leads to improved performance.\n",
    "  * It is flexible and can accommodate various base models.\n",
    "\n",
    "However, \n",
    "  * It may require more computational resources and tuning compared to simpler ensemble methods.\n",
    "  * The final predictions might be harder to interpret because of the complexity resulting from combining multiple models and using a meta-learner.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{admonition} Question\n",
    ":class: important\n",
    "Which type of model is typically used as a meta-model in stacking?\n",
    "\n",
    "```{admonition} Answer\n",
    ":class: tip, dropdown\n",
    "Logistic regression is usually used as meta-learner.\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. <h4>Blending:</h4>\n",
    "\n",
    "* Basic Idea: Blending is a simplified version of stacking that combines the predictions of base models without the need for a separate meta-model. Instead, blending typically uses a simple rule, such as averaging, to combine the predictions. Blending implements **“one-holdout set”**, that is, a small portion of the training data (*validation*) to make predictions which will be “*stacked*” to form the training data of the meta-model.\n",
    "\n",
    "* Training Process: Blending involves training the base models on the original data and then combining their predictions using a predefined rule.\n",
    "* Strengths: Blending is easy to implement and can yield improvements by leveraging the diversity of base models.\n",
    "* Weaknesses: It may not capture complex interactions between base models as effectively as stacking. The choice of the combining rule is critical.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{image} https://dataaspirant.com/wp-content/uploads/2023/03/1-11.png\n",
    ":alt: blending\n",
    ":class: bg-primary mb-1\n",
    ":width: 500px\n",
    ":align: center\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{admonition} Question\n",
    ":class: important\n",
    "What is model blending in the context of machine learning, and how does it differ from model stacking?\n",
    "\n",
    "```{admonition} Answer\n",
    ":class: tip, dropdown\n",
    "Blending is technique derived from stacking and this method involves combining predictions from multiple models using a simple rule or weighted average. It is simpler and often serves as a quick way to combine diverse models.\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Blenging can use meta-model or just apply simple rule like averaging. In further example, we used simple averaging rule.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. <h4>Ensemble Characteristics:</h4>\n",
    "\n",
    "* All three ensembling techniques aim to reduce bias and variance, leading to better generalization performance.\n",
    "* The success of these methods often depends on the diversity of the base models. More diverse models tend to result in better ensembles.\n",
    "* Careful hyperparameter tuning and cross-validation are crucial for optimizing ensemble performance.\n",
    "* In practice, the choice between voting, stacking, and blending depends on the specific problem, dataset, and computational resources available. Each method has its own strengths and weaknesses, and the selection should be guided by the problem's requirements and constraints. Experimentation and testing different ensemble approaches are often necessary to determine the most effective strategy for a particular machine learning task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python implementation\n",
    "\n",
    "The code example that demonstrates how to implement voting, stacking, and blending using scikit-learn for a classification problem on Employee dataset. In this example, we'll use three different base models and ensemble them using these techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual Model 1 Accuracy: 0.8141783029001074\n",
      "Individual Model 2 Accuracy: 0.849624060150376\n",
      "Individual Model 3 Accuracy: 0.7142857142857143\n",
      "Voting Accuracy: 0.8528464017185822\n",
      "Stacking Accuracy: 0.849624060150376\n",
      "Blending Accuracy: 0.8506981740064447\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Read and preprocess Employee dataset\n",
    "df = pd.read_csv(\"../ISLP_datsets/Employee.csv\")\n",
    "\n",
    "en = LabelEncoder()\n",
    "to_encode = ['Education', 'City', 'Gender', 'EverBenched']\n",
    "\n",
    "for col in to_encode:\n",
    "    df[col] = en.fit_transform(df[col])\n",
    "    \n",
    "    \n",
    "X = df.drop(columns={'LeaveOrNot'})\n",
    "y = df.LeaveOrNot\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Initialize base models\n",
    "model1 = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
    "model2 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model3 = LogisticRegression(random_state=42)\n",
    "\n",
    "# Train models\n",
    "model1.fit(X_train, y_train)\n",
    "model2.fit(X_train, y_train)\n",
    "model3.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "pred1 = model1.predict(X_test)\n",
    "pred2 = model2.predict(X_test)\n",
    "pred3 = model3.predict(X_test)\n",
    "\n",
    "# Voting: soft voting\n",
    "voting_model = VotingClassifier(\n",
    "    estimators=[('ada', model1), ('rfc', model2), ('lr', model3)],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "voting_model.fit(X_train, y_train)\n",
    "\n",
    "ensemble_pred_voting = voting_model.predict(X_test)\n",
    "\n",
    "\n",
    "# Stacking\n",
    "stacking_X_train = np.column_stack((pred1, pred2, pred3))\n",
    "stacking_model = LogisticRegression(random_state=42)\n",
    "stacking_model.fit(stacking_X_train, y_test)\n",
    "\n",
    "stacking_X_test = np.column_stack((model1.predict(X_test), model2.predict(X_test), model3.predict(X_test)))\n",
    "ensemble_pred_stacking = stacking_model.predict(stacking_X_test)\n",
    "\n",
    "# Blending\n",
    "\n",
    "# Split the training data further into two parts for blending\n",
    "X_train_base, X_train_blend, y_train_base, y_train_blend = train_test_split(\n",
    "    X_train, y_train, test_size=0.5, random_state=42)\n",
    "\n",
    "model1.fit(X_train_base, y_train_base)\n",
    "model2.fit(X_train_base, y_train_base)\n",
    "model3.fit(X_train_base, y_train_base)\n",
    "\n",
    "# Make predictions using base models on the blending set\n",
    "pred1_blend = model1.predict(X_train_blend)\n",
    "pred2_blend = model2.predict(X_train_blend)\n",
    "pred3_blend = model3.predict(X_train_blend)\n",
    "\n",
    "# Combine predictions of base models on the blending set\n",
    "blending_X_train = np.column_stack((pred1_blend, pred2_blend, pred3_blend))\n",
    "\n",
    "blending_model = LogisticRegression(random_state=42)\n",
    "blending_model.fit(blending_X_train, y_train_blend)\n",
    "\n",
    "pred1_test = model1.predict(X_test)\n",
    "pred2_test = model2.predict(X_test)\n",
    "pred3_test = model3.predict(X_test)\n",
    "\n",
    "blending_X_test = np.column_stack((pred1_test, pred2_test, pred3_test))\n",
    "ensemble_pred_blending = blending_model.predict(blending_X_test)\n",
    "blending_accuracy = accuracy_score(y_test, ensemble_pred_blending)\n",
    "\n",
    "# Evaluate individual models and ensembles\n",
    "individual_model1_accuracy = accuracy_score(y_test, pred1)\n",
    "individual_model2_accuracy = accuracy_score(y_test, pred2)\n",
    "individual_model3_accuracy = accuracy_score(y_test, pred3)\n",
    "voting_accuracy = accuracy_score(y_test, ensemble_pred_voting)\n",
    "stacking_accuracy = accuracy_score(y_test, ensemble_pred_stacking)\n",
    "blending_accuracy = accuracy_score(y_test, ensemble_pred_blending)\n",
    "\n",
    "print(\"Individual Model 1 Accuracy:\", individual_model1_accuracy)\n",
    "print(\"Individual Model 2 Accuracy:\", individual_model2_accuracy)\n",
    "print(\"Individual Model 3 Accuracy:\", individual_model3_accuracy)\n",
    "print(\"Voting Accuracy:\", voting_accuracy)\n",
    "print(\"Stacking Accuracy:\", stacking_accuracy)\n",
    "print(\"Blending Accuracy:\", blending_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion  \n",
    "   *  Model 2 performs the best individually.\n",
    "   *  Combining models through voting, stacking, or blending slightly improves accuracy compared to individual models.\n",
    "   * These ensemble techniques show similar performance, with a slight boost compared to the best individual model (Model 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code:\n",
    "\n",
    "* We load the Employee dataset as an example classification dataset and split it into training and testing sets.\n",
    "* We create three different base models: AdaBoostClassifier, RandomForestClassifier, and LogisticRegression.\n",
    "* Each base model is trained on the training data, and predictions are made on the test data.\n",
    "* We demonstrate three ensemble methods:\n",
    "    * Voting: We aggregate the predictions of the three base models using a soft voting approach for the ensemble.\n",
    "    * Stacking: We use LogisticRegression as a meta-model to learn to combine the predictions of the base models.\n",
    "    * Blending: We employ Logistic Regression to learn and combine predictions from the three base models on hold-out set for the ensemble.\n",
    "\n",
    "Finally, we evaluate the accuracy of the individual models and the ensembles. Note that this is a simplified example, and in practice, you may need to fine-tune hyperparameters and use more diverse base models to achieve optimal performance.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
