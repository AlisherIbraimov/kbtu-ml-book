{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mdainur/kbtu-ml-book/blob/mlp-layers/MLP_team.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6manaTiqtTe2"
      },
      "source": [
        "# Layers of MLP\n",
        "\n",
        "From mathematical point of view MLP is a smooth function $F$ which is constructed as a composition of some other functions\n",
        "\n",
        "$$\n",
        "F(\\boldsymbol x) = (f_{L} \\circ f_{L-1} \\circ\\ldots \\circ f_2 \\circ f_1)(\\boldsymbol x),\\quad\n",
        "\\boldsymbol x \\in \\mathbb R^{n_0}\n",
        "$$\n",
        "\n",
        "Each function\n",
        "\n",
        "$$\n",
        "    f_\\ell \\colon \\mathbb R^{n_{\\ell - 1}} \\to \\mathbb R^{n_\\ell}\n",
        "$$\n",
        "\n",
        "is called a **layer**; it converts representation of $(\\ell-1)$-th layer\n",
        "\n",
        "$$\n",
        "    \\boldsymbol x_{\\ell -1} \\in \\mathbb R^{n_{\\ell - 1}}\n",
        "$$\n",
        "\n",
        "to the representation of $\\ell$-th layer\n",
        "\n",
        "$$\n",
        "   \\boldsymbol x_{\\ell} \\in \\mathbb R^{n_{\\ell}}.\n",
        "$$\n",
        "\n",
        "Thus, the **input layer** $\\boldsymbol x_0 \\in \\mathbb R^{n_0}$ is converted to the **output layer** $\\boldsymbol x_L \\in \\mathbb R^{n_L}$. All other layers $\\boldsymbol x_\\ell$, $1\\leqslant \\ell < L$, are called **hidden layers**. If an MLP has two or more hidden layers, it is called a deep neural network.\n",
        "\n",
        "\n",
        "```{figure} https://www.researchgate.net/publication/354817375/figure/fig2/AS:1071622807097344@1632506195651/Multi-layer-perceptron-MLP-NN-basic-Architecture.jpg\n",
        ":align: center\n",
        "```\n",
        "\n",
        "```{warning}\n",
        "The terminology about layers is a bit ambiguous. Both functions $f_\\ell$ and their outputs $\\boldsymbol x_\\ell = f(\\boldsymbol x_{\\ell - 1})$ are called $\\ell$-th layer in different sources.\n",
        "```\n",
        "\n",
        "## Parameters of MLP\n",
        "\n",
        "However, one important element is missing in this description of MLP: parameters! Each layer $f_\\ell$ has a vector of parameters $\\boldsymbol \\theta_\\ell\\in\\mathbb R^{m_\\ell}$ (sometimes empty). Hence, a layer should be defined as\n",
        "\n",
        "$$\n",
        "    f_\\ell \\colon \\mathbb R^{n_{\\ell - 1}} \\times \\mathbb R^{m_\\ell} \\to \\mathbb R^{n_\\ell}.\n",
        "$$\n",
        "\n",
        "The representation $\\boldsymbol x_\\ell$ is calculated from $\\boldsymbol x_{\\ell -1}$ by the formula\n",
        "\n",
        "$$\n",
        "\\boldsymbol x_\\ell = f_\\ell(\\boldsymbol x_{\\ell - 1},\\boldsymbol \\theta_\\ell)\n",
        "$$\n",
        "\n",
        "with some fixed $\\boldsymbol \\theta_\\ell\\in\\mathbb R^{m_\\ell}$. The whole MLP $F$ depends on parameters of all layers:\n",
        "\n",
        "$$\n",
        "    F(\\boldsymbol x, \\boldsymbol \\theta), \\quad \\boldsymbol \\theta = (\\boldsymbol \\theta_1, \\ldots, \\boldsymbol \\theta_L).\n",
        "$$\n",
        "\n",
        "All these parameters are trained simultaneously by the {ref}`backpropagation method <backprop>`.\n",
        "\n",
        "\n",
        "## Dense layer\n",
        "\n",
        "Edges between two consequetive layers denote **linear** (or **dense**) layer:\n",
        "\n",
        "$$\n",
        "    \\boldsymbol x_\\ell = f(\\boldsymbol x_{\\ell - 1}; \\boldsymbol W, \\boldsymbol b) = \\boldsymbol {Wx}_{\\ell - 1} + \\boldsymbol b.\n",
        "$$\n",
        "\n",
        "The matrix $\\boldsymbol W \\in \\mathbb R^{n_{\\ell - 1}\\times n_\\ell}$ and vector $\\boldsymbol b \\in \\mathbb R^{n_\\ell}$ (bias) are parameters of the linear layer which defines the linear transformation from $\\boldsymbol x_{\\ell - 1}$ to $\\boldsymbol x_{\\ell}$.\n",
        "\n",
        "**Q**. How many numeric parameters does such linear layer have?\n",
        "\n",
        "```{admonition} Exercise\n",
        ":class: important\n",
        "\n",
        "Suppose that we apply one more dense layer:\n",
        "\n",
        "$$\n",
        "    \\boldsymbol x_{\\ell + 1} = \\boldsymbol {W'x}_{\\ell} + \\boldsymbol{b'}\n",
        "$$\n",
        "\n",
        "Express $\\boldsymbol x_{\\ell + 1}$ as a function of $\\boldsymbol x_{\\ell - 1}$.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xC7csZoUtTe7"
      },
      "source": [
        "### Linear layer in PyTorch\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KrMHg1v7tTe8",
        "outputId": "2bef91ad-6c95-4ebb-92d7-eb0bc8ed509e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([-0.0762,  0.6222, -1.1178,  0.9935,  2.0089])"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "x = torch.randn(5)\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_sxAat8ftTe-",
        "outputId": "6718e8bc-9e6b-469a-b8d0-eebe4679f98e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([-0.4328,  0.1590, -0.0809,  0.3756, -0.3382,  0.4353],\n",
              "       requires_grad=True)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "linear_layer = torch.nn.Linear(5, 6)\n",
        "linear_layer.bias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOjGyeFrtTe_"
      },
      "source": [
        "## Activation layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DpI9ljHtTe_"
      },
      "source": [
        "The perceptron computes a single output from multiple real-valued inputs by forming a linear combination according to its input weights and then possibly putting the output through some nonlinear **activation function**. Mathematically this can be written as\n",
        "\n",
        "\n",
        "$$\n",
        "    y = \\boldsymbol\\psi \\Big(\\sum\\limits_{i=1}^l W_i x_i + b \\Big) = \\boldsymbol\\psi( W^\\top \\boldsymbol x + b).\n",
        "$$\n",
        "\n",
        "where, $\\boldsymbol\\psi$ is the activatiion function.\n",
        "\n",
        "```{warning}\n",
        "Note that different layers may have different activation functions.\n",
        "```\n",
        "\n",
        "The original Rosenblatt's perceptron used a Heaviside step function\n",
        "$$\n",
        "    \\mathbb H(x) = \\begin{cases}\n",
        "        1,& \\text{if }  x \\geqslant 0, \\\\\n",
        "        0,& \\text{if }  x < 0.\n",
        "    \\end{cases}\n",
        "$$\n",
        "as the activation function $\\boldsymbol\\phi$. While the value of “1” triggers the activation function and “0” does not. If there exists more than one layer,a value of “1” will be configured to pass the output to the input of the next layer. Consequently, a “0” value is configured to be ignored and will not be passed to the next processor.\n",
        "\n",
        "----- здесь будет Heaviside step function diagram\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nowadays, and especially in multilayer networks, the activation function is often chosen to be the logistic sigmoid\n",
        "$$\n",
        "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
        "$$\n",
        "\n",
        "or the hyperbolic tangent\n",
        "\n",
        "$$\n",
        "\\tanh(x)\n",
        "$$\n",
        "\n",
        "These functions are used because they are mathematically convenient and are close to linear near origin while saturating rather quickly when getting away from the origin. This allows MLP networks to model well both strongly and mildly nonlinear mappings.\n",
        "\n",
        "\n",
        "\n",
        "**Q**. How do the functions of the logistic sigmoid and hyperbolic tangent are related?\n",
        "\n",
        "```{admonition} Exercise\n",
        ":class: important\n",
        "\n",
        "They are related by  \n",
        "$$\n",
        "\\frac{\\tanh(x)+1}2 = \\frac{1}{1 + e^{-2x}}\n",
        "$$\n",
        "\n",
        "```\n",
        "\n",
        "We will discover activation functions in more detail in the {ref}`Activation functions <activations>`."
      ],
      "metadata": {
        "id": "5pmXNS_cQY9I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sigmoid function in PyTorch"
      ],
      "metadata": {
        "id": "OfaQHAj8ZRh3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "x = torch.randn((3, 3, 3))\n",
        "x"
      ],
      "metadata": {
        "id": "38k6IsppRtsR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cda116de-a19e-4760-8da6-83100fa5ea32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-1.5256, -0.7502, -0.6540],\n",
              "         [-1.6095, -0.1002, -0.6092],\n",
              "         [-0.9798, -1.6091, -0.7121]],\n",
              "\n",
              "        [[ 0.3037, -0.7773, -0.0954],\n",
              "         [ 0.1394, -1.5785, -0.3206],\n",
              "         [-0.2993,  1.8793, -0.0721]],\n",
              "\n",
              "        [[ 0.1578,  1.7163, -0.0561],\n",
              "         [ 0.9107, -1.3924,  2.6891],\n",
              "         [-0.1110,  0.2927, -0.1578]]])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = torch.sigmoid(x)\n",
        "\n",
        "y.min(), y.max()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHr9SNgdYvJc",
        "outputId": "ca43887a-64df-4d5a-9e9b-56604f4343b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.1667), tensor(0.9364))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}